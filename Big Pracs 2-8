Practical 2
Aim: map reduce


# Compile the java files
javac -classpath hadoop-core-1.2.1.jar -d wordcountproblem WC_Mapper.java WC.Reducer.java WC_Runner.java

# Create the JAR file
$ jar -cvf wordcountproblem.jar -C wordcountproblem /

# Create a text file in the same folder with some content
$ sudo nano data.txt

# start the Hadoop server
$ start-dfs.sh
$ start-yarn.sh

# Uploading file to Hadoop
1. Go to http://localhost:9870/ > click on utilities > Browse the file system
2. Click on create a new folder and enter name as ‘test’
3. Enter the created folder > click on upload button beside the create new folder button > 
Locate the created text file and open it

# Run the MapReduce program with the JAR file
$ hadoop jar wordcountproblem.jar com.wordcountproblem.WC_Runner /test/data.txt /r_output

# print the generated output to the console
$ hdfs dfs -cat /r_output/part-00000

-----------------------------------------------------------------------------------------------------
pra -3
 aim: scala

# Scala installation
$ sudo apt install scala

# check scala installation
$ scala –version

# Spark installation
1. Extract the Spark (make sure to be in the directory the spark is downloaded)
$ sudo tar -xvf spark-3.2.1-bin-hadoop3.2.tgz

2. Create an installation directory /opt/spark.
$ sudo mkdir /opt/spark

3. Move the extracted files to the installation directory.
$ sudo mv spark-3.2.0-bin-hadoop3.2/* /opt/spark

4. Change the permission of the directory.
$ sudo chmod -R 777 /opt/spark

5. Edit the bashrc configuration file to add Apache Spark installation directory to the system path.
$ sudo nano ~/.bashrc

6. Add the code below at the end of the file, save and exit the file:
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

7. Save the changes to take effect.
$ source ~/.bashrc

8. Start the standalone master server.
9. $ start-master.sh

10. Find your server hostname from the dashboard by visiting http://Localhost:8080. Under the URL 
value. It might look like this:
Spark://Ubuntu.myguest.virtualbox.org:7077

11. Start the Apache Spark worker process. Change spark://ubuntu:7077 with your server 
hostname.
$ start-worker.sh Spark://Ubuntu.myguest.virtualbox.org:7077

12. Use jps to confirm the status

13. Type spark-shell to access the shell


# Entering graphical data in spark shell

#creating graphical data in graphx

import org.apache.spark.graphx._

# creating own data type
case class User(name: String, age: Int)
val users = List((1L, User("Alex", 26)), (2L, User("Bill", 42)), (3L, User("Carol", 18)), (4L, User("Dave", 16)), 
(5L, User("Eve", 45)), (6L, User("Farell", 30)), (7L, User ("Garry", 32)), (8L, User("Harry", 36)), (9L, 
User("Ivan", 28)), (10L, User("Jill", 48)))
val usersRDD = sc.parallelize (users)
val follows = List(Edge(1L, 2L, 1), Edge(2L, 3L, 1), Edge(3L, 1L, 1), Edge(3L, 4L, 1), Edge(3L, 5L, 1), Edge(4L, 
5L, 1), Edge(6L, 5L, 1), Edge(7L, 6L, 1), Edge(6L, 8L, 1), Edge(7L, 8L, 1), Edge(7L, 9L, 1), Edge(9L, 8L, 1), 
Edge(8L, 10L, 1), Edge(10L, 9L, 1), Edge(1L, 1L, 1))
val followsRDD = sc.parallelize(follows)

# creating user to access data 19
val defaultUser = User("Icarus", 22)
val socialgraph = Graph (usersRDD, followsRDD, defaultUser)

#Access data of the graph

socialgraph.numEdges
socialgraph.numVertices
socialgraph.inDegrees.collect
socialgraph.outDegrees.collect

--------------------------------------------------------------------------------------------------------------------

Practical No. 4
Aim: Write a Spark code for the given application and handle error

# switch to Hadoop user
# Install scala-sbt via the latest command available in the terminal

**
echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/so
urces.list.d/sbt.list
echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | sudo tee /etc/apt/sources.l
ist.d/sbt_old.list
curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2
DF73499E82A75642AC823" | sudo apt-key add
sudo apt-get update
sudo apt-get install sbt
**

# enter sbt in the terminal to verify installation
# create the scala program for exception handling
$ nano ExceptionHandlingTest.scala

**
import org.apache.spark.sql.SparkSession
object ExceptionHandlingTest {
 def main(args: Array[String]): Unit = {
 val spark = SparkSession
 .builder
 .appName("ExceptionHandlingTest")
 .getOrCreate()
 spark.sparkContext.parallelize(0 until 
spark.sparkContext.defaultParallelism).foreach { i =>
if (math.random > 0.75) {
 throw new Exception("Testing exception handling")
 }
 }
 spark.stop()
 }
}
**

# create the sbt dependency file ‘exceptionhandlingtest.sbt’
name := "exceptionhandlingtest"
version := "1.0"
scalaVersion := "2.12.15"
val sparkVersion = "3.3.1"
libraryDependencies ++= Seq(
 "org.apache.spark" %% "spark-core" % sparkVersion,
 "org.apache.spark" %% "spark-sql" % sparkVersion
 )
# start spark master & worker
$ spark-master.sh
$ spark-worker.sh spark://Ubuntu.myguest.virtualbox.org:7077/
# go back to the terminal pointing to the directory with scala and sbt file and create the sbt 
package
$ sbt package
# submit the program to spark
$ spark-submit –class ‘ExceptionHandlingTest’ –master 
‘spark://Ubuntu.myguest.virtualbox.org:7077/’ \target/scala-2.12/exceptionhandling-2.12-1.0.jar

output:


-------------------------------------------------------------------------------------------------------

Practical No. 5
Aim: Write a Spark code to handle the Streaming of data.

# sbt package creation 
1. Create a new folder logged in as Hadoop user
2. Create file ‘NetworkWordCount.scala’ in that folder
import org.apache.spark.SparkConf
import org.apache.spark.streaming._
object NetworkWordCount {
 def main(args: Array[String]): Unit = {
 val sparkConf = new 
SparkConf().setMaster("local[2]")setAppName("NetworkWordCount")
 val ssc = new StreamingContext(sparkConf, Seconds(10))
 val lines = ssc.socketTextStream("localhost",9999)
 val words = lines.flatMap(_.split(" "))
 val tuples = words.map(word => (word ,1))
 val wordCounts = tuples.reduceByKey((t, v) => t + v)
 wordCounts.print()
 ssc.start()
 ssc.awaitTermination()
 }
}
3. Create a ‘networkwordcount.sbt’ file in the same folder
name := "networkwordcount"
version := "1.0.0"
scalaVersion := "2.12.15"
libraryDependencies += "org.apache.spark" % "spark-streaming_2.12" % "3.3.1" % 
"provided"
4. Create the sbt package
$ sbt package
5. Start the spark server
$ start-master.sh
$ start-worker.sh spark://Ubuntu.myguest.virtualbox.org:7077/


6. On a separate terminal, start the netscape server
$ nc –lk 9999
7. On the original terminal, submit the scala program to spark
$ spark-submit –class ‘NetworkWordCount –master 
‘spark://Ubuntu.myguest.virtualbox.org:7077/’ \target/scala2.12/networkwordcount_2.12-1.0.0.jar
8. On the netscape terminal provide textual input for the scala program to count words of the live 
stream every 10 seconds


-------------------------------------------------------------------------------------------------------------

Practical No. 6
Aim: Install Hive and use Hive Create and store structured databases.

# Hive installation
1. Download hive-2.3.9 from the given link 
https://downloads.apache.org/hive/
2. Extract, rename and move the downloaded zip file to the appropriate folder
3. Edit the .bashrc file
$ nano ~/.bashrc
export HIVE_HOME=/home/hadoop/hive
export PATH=$PATH:$HIVE_HOME/bin
4. Edit the core-site.xml and add the following properties within the existing Hadoop configuration
$ nano $HADOOP_HOME/etc/Hadoop/core-site.xml
<property>
<name>hadoop.proxyuser.hadoop.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.hadoop.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.server.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.server.groups</name>
<value>*</value>
</property>
5. Make the hdfs directory
$ hadoop fs –mkdir /tmp

$ hadoop fs –mkdir /tmp/user
$ hadoop fs –mkdir /tmp/user/hive
$ hadoop fs –mkdir /tmp/user/hive/warehouse
6. Give the permissions
$ hadoop fs -chmod g+w /tmp
$ hadoop fs -chmod g+w tmp/user/hive/warehouse
7. Initialize the derby database
$ schematool -dbType derby -initSchema
8. Start the hiveserver2
$ hiveserver2
9. Open a new terminal and connect beeline with hive server
$ beeline -n hadoop -u jdbc:hive2://localhost:10000
# Create and store data
1. Create a new database
$ create database test;
# verify with
$ show databases;
2. Create a new table
$ create table test.emp (id int, name string);
3. Insert a few tuples/records in the created table
$ insert into test.emp VALUES(1, ‘Andy’);
$ insert into test.emp VALUES(2, ‘Ramy’);
$ insert into test.emp VALUES(3, ‘Youka’);
4. Display the table data
$ select * from test.emp

-----------------------------------------------------------------------------------------------------

Practical No.7
Aim: Install HBase and use the HBase Data model Store and retrieve data.

#HBase installation
1. Go to the official site and download the most stable version of hbase 
2. Extract the zip file and place it in Hadoop home directory
3. Open hbase-env.sh in hbase/conf and assign the JAVA_HOME path
$ sudo nano hbase/conf/hbase-env.sh
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
4. Edit the .bashrc file
$ nano ~/.basrhc
export HBASE_HOME=/home/hbuser/hbase
export PATH= $PATH:$HBASE_HOME/bin
5. Read the edited bashrc file to the running memory
$ source ~/.bashrc
6. Add the following properties below the existing ones in the hbase/conf/hbase-site.xml file
<property>
<name>hbase.rootdir</name>
<value>file:///home/hbuser/hbase</value>
</property>
<property>
<name>hbase.zookeeper.property.dataDir</name>
<value>/home/hbuser/hbase</value>
</property>
7. Run hbase by typing
$ start-hbase.sh
$ hbase shell


# Storing and retrieval of data 
# Enter the following command in the hbase shell
Create ‘emp’, ‘pri_data’, ‘pro_data’
Put ‘emp’, ‘1’, ‘pri_data:name’, ‘Andy’
e =get_table
e.put ‘1’, ‘pri_data:age’, ‘22’
e.put ‘1’, ‘pro_data:post’, ‘asst. manager’
e.put ‘1’, ‘pro_data:salary’, ‘40k’
e.put ‘2’, ‘pri_data:name’, ‘Icarus’
e.put ‘2’, ‘pri_data:age’, ‘22’
e.put ‘2’, ‘pro_data:post’, ‘manager’
e.get ‘1’
e.get ‘2’

Output:


-----------------------------------------------------------------------------------------------------------------
Practical No. 8
Aim: Write a Pig Script for solving counting problems.

# Pig installation

1. Download the zip file from the official Pig release site
2. Extract the pig zip
$ sudo tar –xf pig-0-17.0.tar.gx
3. Moving the file
$ mv pig-0-17-0 /home/Hadoop/pig
4. Set the .bashrc file
export PIG_HOME=/home/hadoop/pig
export PATH =$PATH:/home/hadoop/pig/bin
export PIG_CLASSPATH=$HADOOP_HOME/conf
5. Verify installation
$ pig –version
6. Run pig locally
$ pig –x local


# Pig script for word count problem
1. Create a text file in /home/Hadoop/textfile.txt path and provide content
$ nano /home/hadoop/textfile.txt

2. Enter the following command in the terminal
lines = LOAD ‘/home/hadoop/textfile.txt’ AS (line:chararray);
words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;
grouped = GROUP words BY word;
wordcount = FOREACH grouped GENERATE group, COUNT(words);
DUMP wordcount;




